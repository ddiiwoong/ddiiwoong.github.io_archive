---
layout: post
title: "Cilium"
comments: true
description: "Cilium에 대해 알아봅니다"
keywords: "Cilium, Kubernetes, k8s, network policy, minikube, istio, BPF, Networking and Security"
---

## Cilium 누구냐 넌?
처음에 놀랐다. 번역하면 '자궁', '섬모', '속눈썹'등 익숙치 않은 단어라서 놀랐지만 차근히 보다보니 결국 OS내부(리눅스 커널)에서부터 컨테이너간 또는 외부와의 연결을 보호하는 역할이라고 하니 이해가 되는 것 같다.

홈페이지에 정의된 내용을 보면 아래 내용과 같다. 

>[Cilium](https://cilium.readthedocs.io/en/v1.1/intro/#what-is-cilium) - Docker 및 Kubernetes와 같은 Linux 컨테이너 관리 플랫폼을 사용하여 배포된 응용 프로그램 서비스 간의 네트워크 연결을 보호하는 오픈 소스 소프트웨어

페북에 [한국 리눅스 사용자 그룹](https://www.facebook.com/groups/korelnxuser/)에서도 [Tommy Lee](https://www.facebook.com/tommy.lee.98229)님이나 [송창안](https://www.facebook.com/changan.song)님이 몇몇 게시물을 올려주셔서 관심있게 보던중에 아래와 같은 내용을 보고 이거다 하고 파보기 시작했다.

>Cilium은 Docker 및 Kubernetes와 같은 리눅스 컨테이너 프레임 워크에 API 기반의 네트워크 보안 필터링을 제공하며, 
또한, BPF라는 새로운 Linux 커널 기술을 사용하여 컨테이너/POD ID를 기반으로 네트워크 계층 및 응용 프로그램 계층 보안 정책을 정의 및 적용하는 것에 있어서 간단하면서 효율적인 방법을 제공하고 있습니다.

윗분들처럼 커널 자체를 분석하고 공부하고 기여하려면 소요되는 시간이 더 걸릴거 같고 서비스를 운영하고 개발하는 입장에서는 내 프로젝트에 적용 가능성을 검증하는것이 나을것 같아 정리를 해본다.

iptables을 기반으로 IP와 Port기반의 전통적인 포워딩 기술은 벌써 20년이라는 세월동안 널리 사용되어 왔다. 특히 퍼블릭/프라이빗 클라우드 제품군들 모두 iptables기반의 Security Group등을 기본으로 제공하고 있고 Kubernetes 마저도 CNI 핵심으로 iptables을 활용하고 있다.  

동적으로 변화하고 매우 복잡한 마이크로서비스를 사용하는 시대에 전통적인 방식의 IP, Port관리는 비효율적인 측면이 없지 않다. BPF(아래인용)을 활용하여 리눅스 커널내에서 데이터 포워딩을 할 수 있고 Kubernetes Service기반 Load Balancing이나 istio와 같은 Service Mesh를 위한 Proxy Injection 을 통해 여러 활용을 할 수 있을거라고 Cilium 프로젝트는 이야기 하고 있다. 

> 버클리 패킷 필터(Berkeley Packet Filter, BPF)
> 
>BPF는 버클리 패킷 필터(Berkeley Packet Filter)의 줄임말이다. 이름 그대로 패킷을 걸러내는 필터이다. 그런데 BSD에서의 BPF는 네트워크 탭(리눅스의 PF_PACKET)까지 아우르는 개념이다. 옛날 옛적에 유닉스에는 CSPF(CMU/Stanford Packet Filter)라는 게 있었는데 BPF라는 새 구조가 이를 대체했다. 이후 리눅스에서는 네트워크 탭을 나름의 방식으로 구현하고 패킷 필터 부분만 가져왔다. 리눅스의 패킷 필터를 리눅스 소켓 필터링(LSF: Linux Socket Filtering)이라고도 한다.  
(발췌) https://wariua.github.io/facility/extended-bpf.html 

Cilium은 Dockercon 2017에서 최초 [announce](https://www.youtube.com/watch?v=ilKlmTDdFgk)를 하였고 2018년 4월 24일에 1.0이 정식 Release된 이후 많은 관심을 받을것으로 예상되어 실제 서비스에 적용해볼 필요가 있을거 같아 minikube로 테스트한 내용을 끄젹여 본다. 

## Cilium Architecture
![Cilium Architecture](/images/cilium_arch.png)

## Main Feature
* 고효율 BPF Datapath  
  * 모든 데이터 경로가 클러스터 전체에 완전 분산
  * Envoy같은 proxy injection 제공, 추후 sidecar proxy 형태 제공예정
* CNI, CMM plugins  
  * Kubernetes, Mesos, Docker에 쉽게 통합가능
* Packet, API 네트워크 보안  
  패킷기반 네트워크 보안과 API 인증을 결합하여 전통적인 패킷기반 네트워크 보안과 마이크로서비스 아키텍처 모두에게 보안 제공가능  
  * [ID기반](http://docs.cilium.io/en/doc-1.0/concepts/#arch-id-security) - Source IP에만 의존하지 않고 모든패킷에 workload identity를 encoding하여 식별성 강화  
  * [IP/CIDR기반](http://docs.cilium.io/en/doc-1.0/policy/language/#ip-cidr-based)이외에도 [Kubernetes Service기반](http://docs.cilium.io/en/doc-1.0/policy/language/#services-based)으로 정책 설정가능
  * L7기반 [API보안](http://docs.cilium.io/en/doc-1.0/policy/language/#layer-7-examples) 적용가능
* 분산,확장가능한 Load Balacing
  BPF를 사용한 고성능 L3,L4 Load Balancer 제공
  (Hasing, Weighted round-robin)
  * kube-proxy 대체 - Kubernetes ClusterIP가 생성될때 BPF기반으로 자동으로 적용됨 
  * [API driven](http://docs.cilium.io/en/doc-1.0/api/) - 직접 API를 활용하여 확장가능
* 단순화된 네트워크 모델  
  Overlay/VXLAN, Direct/Native Routing 지원
* 가시성  
   * [Microscope](https://github.com/cilium/microscope) - 클러스터 레벨에서 모든 이벤트 필터링 가능
   * API기반 가시성 제공
* 운영
  * 클러스터 헬스체크 - HTTP, ICMP기준 클러스터 latency 체크가능
  * Prometheus 통합 - 모든 메트릭을 Prometheus로 전달가능
  * 클러스터 분석 및 [리포트툴](http://docs.cilium.io/en/doc-1.0/troubleshooting/#cluster-diagnosis-tool) 

## Requirement
- kubectl >= 1.7.0
- minikube >= 0.22.3

[kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 설치와 [minikube](https://github.com/kubernetes/minikube/releases) 구성은 별도로 해야한다.

## Start minikube
넉넉하게 4GB 이상 메모리로 minikube를 구동한다.
```
$ minikube start --kubernetes-version v1.9.0 --network-plugin=cni --extra-config=kubelet.network-plugin=cni --memory=5120 
```

## Check minikube cluster status
minikube구성이 완료되면 아래와 같이 클러스터 상태를 확인할수 있다. 
```
$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}
```

## Install etcd (dependency of cilium)
cilium 의존성을 위해 etcd를 별도로 배포한다. 
```
$ kubectl create -n kube-system -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes/addons/etcd/standalone-etcd.yaml  
service "etcd-cilium" created
statefulset.apps "etcd-cilium" created
```

## Check all pods (etcd)
모든 pod가 ```Running``` 상태인지 확인한다.
```
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE
kube-system   etcd-cilium-0                      1/1       Running   0          1m
kube-system   etcd-minikube                      1/1       Running   0          3m
kube-system   kube-addon-manager-minikube        1/1       Running   0          4m
kube-system   kube-apiserver-minikube            1/1       Running   0          3m
kube-system   kube-controller-manager-minikube   1/1       Running   0          3m
kube-system   kube-dns-86f4d74b45-lhzfv          3/3       Running   0          4m
kube-system   kube-proxy-tcd7h                   1/1       Running   0          4m
kube-system   kube-scheduler-minikube            1/1       Running   0          4m
kube-system   storage-provisioner                1/1       Running   0          4m
```

## Install Cilium
Kubernetes 클러스터에 Cilium을 인스톨한다. 기본적으로 DaemonSet 형태로 배포되기 때문에 Node당 한개의 Cilium Pod를 볼 수 있다. Cilium은 ```kube-system``` namespace에서 실행된다.
```
$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes/1.9/cilium.yaml
configmap "cilium-config" created
secret "cilium-etcd-secrets" created
daemonset.extensions "cilium" created
clusterrolebinding.rbac.authorization.k8s.io "cilium" created
clusterrole.rbac.authorization.k8s.io "cilium" created
serviceaccount "cilium" created
```
kube-system namespace에 RBAC설정과 함께 Cilium이 배포되고, ConfigMap, DaemonSet 형태로 배포가 된다. 

## Check deployment

Cilium Deployment가 ```READY``` 상태로 바뀔때까지 기다린다.


```
$ kubectl get daemonsets -n kube-system
NAME      DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
cilium    1         1         1         1            0           <none>          2m
```

## Install Istio 

[Helm Client](https://docs.helm.sh/using_helm/#installing-helm) 설치

Istio 0.8.0 Download
```
$ export ISTIO_VERSION=0.7.0
$ curl -L https://git.io/getLatestIstio | sh -
$ export ISTIO_HOME=`pwd`/istio-${ISTIO_VERSION}
$ export PATH="$PATH:${ISTIO_HOME}/bin"
```

kubernetes 클러스터에 Cilium network policy injection을 위한 설정을 변경하고 istio를 배포한다.
```
$ sed -e 's,docker\.io/istio/pilot:,docker.io/cilium/istio_pilot:,' \
      < ${ISTIO_HOME}/install/kubernetes/istio.yaml | \
      kubectl create -f -

$ sed -e 's,#interceptionMode: .*,interceptionMode: TPROXY,' \
      -i istio-cilium-helm/templates/configmap.yaml
```

이때 macOS의 경우 sed stdin error가 발생할수 있기 때문에 아래와 같이 gnu-sed를 설치한후 ```sed``` 명령을 재수행 한다.

```
$ brew install gnu-sed
$ PATH="/usr/local/opt/gnu-sed/libexec/gnubin:$PATH"
$ MANPATH="/usr/local/opt/gnu-sed/libexec/gnuman:$MANPATH"
```

커스터마이징을 위해 istio Helm chart에서 Copy하고
Cilium network policy injection을 위해 deploy설정을 변경한다.  
```
$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/cilium-kube-inject.awk > cilium-kube-inject.awk

$ cat istio-cilium-helm/templates/sidecar-injector-configmap.yaml | \
      awk -f cilium-kube-inject.awk \
      > istio-cilium-helm/templates/sidecar-injector-configmap-cilium.yaml
$ mv istio-cilium-helm/templates/sidecar-injector-configmap-cilium.yaml istio-cilium-helm/templates/sidecar-injector-configmap.yaml
```


Istio와 Envoy에서 Cilium Proxy docker image를 사용하고 Cilium Unix Socket을 사용하도록 해야하기 때문에 아래와 같이 Istio sidecar inject template을 수정해야 한다.

```
$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/cilium-kube-inject.awk > cilium-kube-inject.awk

$ cat istio-cilium-helm/templates/sidecar-injector-configmap.yaml | \
    awk -f cilium-kube-inject.awk \
    > istio-cilium-helm/templates/sidecar-injector-configmap-cilium.yaml
$ mv istio-cilium-helm/templates/sidecar-injector-configmap-cilium.yaml istio-cilium-helm/templates/sidecar-injector-configmap.yaml
```

istio deployment spec을 생성한다.
```
$ helm template istio-cilium-helm --name istio --namespace istio-system \
      --set sidecarInjectorWebhook.enabled=false \
      --set global.controlPlaneSecurityEnabled=false \
      --set global.mtls.enabled=false \
      --set global.proxy.image=proxy_debug \
      > istio-cilium.yaml
```

Istio 배포 (CustomResource 엄청많다...)
```
$ kubectl create namespace istio-system
$ kubectl create -f istio-cilium.yaml
configmap "istio-statsd-prom-bridge" created
configmap "istio-mixer-custom-resources" created
configmap "prometheus" created
configmap "istio" created
configmap "istio-sidecar-injector" created
serviceaccount "istio-egressgateway-service-account" created
serviceaccount "istio-ingress-service-account" created
serviceaccount "istio-ingressgateway-service-account" created
serviceaccount "istio-mixer-post-install-account" created
clusterrole.rbac.authorization.k8s.io "istio-mixer-post-install-istio-system" created
clusterrolebinding.rbac.authorization.k8s.io "istio-mixer-post-install-role-binding-istio-system" created
job.batch "istio-mixer-post-install" created
serviceaccount "istio-mixer-service-account" created
serviceaccount "istio-pilot-service-account" created
serviceaccount "prometheus" created
serviceaccount "istio-citadel-service-account" created
serviceaccount "istio-cleanup-old-ca-service-account" created
customresourcedefinition.apiextensions.k8s.io "rules.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "attributemanifests.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "circonuses.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "deniers.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "fluentds.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "kubernetesenvs.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "listcheckers.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "memquotas.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "noops.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "opas.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "prometheuses.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "rbacs.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "servicecontrols.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "solarwindses.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "stackdrivers.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "statsds.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "stdios.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "apikeys.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "authorizations.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "checknothings.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "kuberneteses.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "listentries.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "logentries.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "metrics.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "quotas.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "reportnothings.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "servicecontrolreports.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "tracespans.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "serviceroles.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "servicerolebindings.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "destinationpolicies.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "egressrules.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "routerules.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "virtualservices.networking.istio.io" created
customresourcedefinition.apiextensions.k8s.io "destinationrules.networking.istio.io" created
customresourcedefinition.apiextensions.k8s.io "serviceentries.networking.istio.io" created
customresourcedefinition.apiextensions.k8s.io "gateways.networking.istio.io" created
customresourcedefinition.apiextensions.k8s.io "policies.authentication.istio.io" created
customresourcedefinition.apiextensions.k8s.io "httpapispecbindings.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "httpapispecs.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "quotaspecbindings.config.istio.io" created
customresourcedefinition.apiextensions.k8s.io "quotaspecs.config.istio.io" created
clusterrole.rbac.authorization.k8s.io "istio-ingress-istio-system" created
clusterrole.rbac.authorization.k8s.io "istio-mixer-istio-system" created
clusterrole.rbac.authorization.k8s.io "istio-pilot-istio-system" created
clusterrole.rbac.authorization.k8s.io "prometheus-istio-system" created
clusterrolebinding.rbac.authorization.k8s.io "prometheus-istio-system" created
clusterrole.rbac.authorization.k8s.io "istio-citadel-istio-system" created
role.rbac.authorization.k8s.io "istio-cleanup-old-ca-istio-system" created
clusterrolebinding.rbac.authorization.k8s.io "istio-ingress-istio-system" created
clusterrolebinding.rbac.authorization.k8s.io "istio-mixer-admin-role-binding-istio-system" created
clusterrolebinding.rbac.authorization.k8s.io "istio-pilot-istio-system" created
clusterrolebinding.rbac.authorization.k8s.io "istio-citadel-istio-system" created
rolebinding.rbac.authorization.k8s.io "istio-cleanup-old-ca-istio-system" created
service "istio-egressgateway" created
service "istio-ingress" created
service "istio-ingressgateway" created
service "istio-policy" created
service "istio-telemetry" created
service "istio-statsd-prom-bridge" created
deployment.extensions "istio-statsd-prom-bridge" created
service "istio-pilot" created
service "prometheus" created
service "istio-citadel" created
deployment.extensions "istio-egressgateway" created
deployment.extensions "istio-ingress" created
deployment.extensions "istio-ingressgateway" created
deployment.extensions "istio-policy" created
deployment.extensions "istio-telemetry" created
deployment.extensions "istio-pilot" created
deployment.extensions "prometheus" created
deployment.extensions "istio-citadel" created
job.batch "istio-cleanup-old-ca" created
horizontalpodautoscaler.autoscaling "istio-egressgateway" created
horizontalpodautoscaler.autoscaling "istio-ingress" created
horizontalpodautoscaler.autoscaling "istio-ingressgateway" created
```

배포상태 확인  
모든 Istio pod가 준비되면 application 배포 테스트를 해보자.
```
$ kubectl get deployment -n istio-system
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
istio-citadel              1         1         1            0           20s
istio-egressgateway        1         1         1            0           20s
istio-ingress              1         1         1            0           20s
istio-ingressgateway       1         1         1            0           20s
istio-pilot                1         1         1            0           20s
istio-policy               1         1         1            0           20s
istio-statsd-prom-bridge   1         1         1            0           20s
istio-telemetry            1         1         1            0           20s
prometheus                 1         1         1            0           20s
```


## Bookinfo App. V1 배포하기

Istio로 샘플앱 배포하는 것은 [공식문서](https://istio.io/docs/guides/bookinfo.html)를 직접 확인하거나 [마이크로서비스 샘플앱(BookInfo) 배포하기](https://yunsangjun.github.io/blog/istio/2018/04/26/deploying-bookinfo-on-kubernetes.html)를 참고하면 도움이 된다.

Bookinfo에 대한 자세한 설명은 생략하기로 하고 istio-sidecar 방식으로 배포해보자. 여기선 일단 [MutatingWebhook](https://ddiiwoong.github.io/2018/mutating-web-hook/)은 사용하지 않는다.

### Bookinfo App. 구성요소 및 구성도
* Kubernetes Service
* Kubernetes Deployment
* Cilium Network Policy (limiting traffic)

![istio-bookinfo](http://cilium.readthedocs.io/en/latest/_images/istio-bookinfo-v1.png)

Pod가 시작될때 필요한 Istio sidecar proxy정책을 만든다.

```
$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/istio-sidecar-init-policy.yaml
ciliumnetworkpolicy "istio-sidecar" created
```

ingress gateway 배포
단일이 아닌 multi cluster로 운영중이면 반드시 istioctl -c <kubeconfig_path> 옵션을 주고 실행해야 한다. 
```
$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/bookinfo-productpage-ingress.yaml | \
      istioctl create -c /Users/ddii/.kube/config -f -
Created config gateway/default/productpage at revision 17808
Created config virtual-service/default/productpage at revision 17809
```

마찬가지로 서비스와 App을 배포할때도 injection을 위해 istioctl -c <kubeconfig_path> 옵션을 주고 실행한다. MacOS에서는 공식 Docs대로 하면 For문 에러가 나므로 아래와 같이 별도로 실행한다.
```
$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/bookinfo-productpage-service.yaml | \
      istioctl kube-inject -c /Users/ddii/.kube/config -f - | \
      kubectl create -f -
service/productpage created

$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/bookinfo-productpage-v1.yaml | \
      istioctl kube-inject -c /Users/ddii/.kube/config -f - | \
      kubectl create -f -
ciliumnetworkpolicy.cilium.io/productpage-v1 created
deployment.extensions/productpage-v1 created

$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/bookinfo-details-v1.yaml | \
      istioctl kube-inject -c /Users/ddii/.kube/config -f - | \
      kubectl create -f -
service/details created
ciliumnetworkpolicy.cilium.io/details-v1 created
deployment.extensions/details-v1 created

$ curl -s https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes-istio/bookinfo-reviews-v1.yaml | \
      istioctl kube-inject -c /Users/ddii/.kube/config -f - | \
      kubectl create -f -
service/reviews created
ciliumnetworkpolicy.cilium.io/reviews-v1 created
deployment.extensions/reviews-v1 created
```

BookInfo App. 배포 확인.
``` 
$ kubectl get deployments -n default
NAME             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
details-v1       1         1         1            1           6m
productpage-v1   1         1         1            1           6m
reviews-v1       1         1         1            1           6m
```

